{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Smote Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE: stroke\n",
      "0    3889\n",
      "1    3889\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubham\\AppData\\Local\\Temp\\ipykernel_16816\\1725182024.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['bmi'].fillna(data['bmi'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset (replace 'healthcare-dataset-stroke-data.csv' with your file path)\n",
    "data = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "\n",
    "# Drop 'id' column if present\n",
    "if 'id' in data.columns:\n",
    "    data = data.drop('id', axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "data['bmi'].fillna(data['bmi'].median(), inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "for col in categorical_cols:\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Create derived features\n",
    "data['age_glucose'] = data['age'] * data['avg_glucose_level']\n",
    "data['comorbidity'] = data['hypertension'] | data['heart_disease']\n",
    "data['age_group'] = pd.cut(data['age'], bins=[0, 40, 60, 120], labels=[0, 1, 2], include_lowest=True)\n",
    "data['age_group'] = data['age_group'].astype(int)\n",
    "\n",
    "# Define features and target\n",
    "X = data[['age', 'gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type',\n",
    "          'avg_glucose_level', 'bmi', 'smoking_status', 'age_glucose', 'comorbidity', 'age_group']]\n",
    "y = data['stroke']\n",
    "\n",
    "# Split the data (before balancing to avoid data leakage)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_smote_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New scaler saved as 'scaler_smote_new.pkl'\n"
     ]
    }
   ],
   "source": [
    "# import joblib\n",
    "# joblib.dump(scaler, 'scalers/Logistic_scaler_smote_new.pkl')\n",
    "# print(\"New scaler saved as 'scaler_smote_new.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression (SMOTE) Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.83      0.90       972\n",
      "           1       0.17      0.68      0.28        50\n",
      "\n",
      "    accuracy                           0.82      1022\n",
      "   macro avg       0.58      0.76      0.59      1022\n",
      "weighted avg       0.94      0.82      0.87      1022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train Logistic Regression with scaled data\n",
    "lr_model_smote = LogisticRegression(class_weight='balanced', random_state=42, max_iter=2000)\n",
    "lr_model_smote.fit(X_train_smote_scaled, y_train_smote)\n",
    "\n",
    "# Predict on the test set\n",
    "lr_pred_smote = lr_model_smote.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nLogistic Regression (SMOTE) Test Set Performance:\")\n",
    "print(classification_report(y_test, lr_pred_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression (SMOTE) model saved as 'logistic_regression_smote_tuned_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# # Save the tuned Logistic Regression model\n",
    "# joblib.dump(lr_model_smote, 'models1/logistic_regression_smote_model.pkl')\n",
    "# print(\"Tuned Logistic Regression (SMOTE) model saved as 'logistic_regression_smote_tuned_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C (Inverse of Regularization Strength)\n",
    "Controls the trade-off between fitting the training data and keeping the model simple (to prevent overfitting).\n",
    "\n",
    "Smaller values of C increase regularization (simpler model, less overfitting), while larger values reduce regularization (more complex model, better fit to training data).\n",
    "\n",
    "Weâ€™ll test a range of values to find the best balance.\n",
    "\n",
    "solver (different fitters)\n",
    "\n",
    "class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Parameters: {'C': 0.1, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best F1-Score (Class 1) on Cross-Validation: 0.8407496018409976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shubham\\Documents\\ds_mini\\mini\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "50 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Shubham\\Documents\\ds_mini\\mini\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Shubham\\Documents\\ds_mini\\mini\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Shubham\\Documents\\ds_mini\\mini\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Shubham\\Documents\\ds_mini\\mini\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Shubham\\Documents\\ds_mini\\mini\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.82686531 0.82034946 0.82766737 0.82879657 0.82781171\n",
      "        nan 0.78370013 0.7819623  0.77938795 0.78337267 0.77954463\n",
      "        nan 0.84047267 0.8407496  0.83872229 0.83877509 0.83873684\n",
      "        nan 0.79209733 0.79201682 0.79100143 0.79119768 0.79075708\n",
      "        nan 0.83979545 0.83990205 0.83936633 0.83936633 0.83936633\n",
      "        nan 0.79315704 0.79307292 0.79331982 0.7929928  0.7929928\n",
      "        nan 0.83944919 0.83959315 0.83969976 0.83959315 0.83959315\n",
      "        nan 0.79340267 0.79323783 0.79356656 0.79323783 0.79323783\n",
      "        nan 0.83944919 0.83959315 0.83959315 0.83959315 0.83959315\n",
      "        nan 0.79340267 0.79323783 0.79356656 0.79323783 0.79323783]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Range of regularization strengths\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga'],  # Different solvers\n",
    "    'penalty': ['l1', 'l2'],  # L1 and L2 penalties (we'll handle solver-penalty compatibility in GridSearchCV)\n",
    "    'class_weight': ['balanced', {0: 1, 1: 10}]  # Balanced vs. custom weight for minority class\n",
    "}\n",
    "\n",
    "# Handle solver-penalty compatibility (e.g., 'lbfgs' only supports 'l2')\n",
    "# GridSearchCV will automatically skip incompatible combinations\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=2000, random_state=42)\n",
    "\n",
    "# Define a custom scorer for F1-score of the minority class (class 1)\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lr_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=f1_scorer,  # Optimize for F1-score of the stroke class\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_smote_scaled, y_train_smote)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-Score (Class 1) on Cross-Validation:\", grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparameters Logistic Regression (SMOTE, Tuned) Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.83      0.90       972\n",
      "           1       0.17      0.70      0.27        50\n",
      "\n",
      "    accuracy                           0.82      1022\n",
      "   macro avg       0.58      0.76      0.59      1022\n",
      "weighted avg       0.94      0.82      0.87      1022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from GridSearchCV\n",
    "best_lr_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "lr_pred_tuned = best_lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nBest hyperparameters Logistic Regression (SMOTE, Tuned) Test Set Performance:\")\n",
    "print(classification_report(y_test, lr_pred_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubham\\AppData\\Local\\Temp\\ipykernel_16816\\1637931033.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['bmi'].fillna(data['bmi'].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost (SMOTE, Default Threshold) Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.93       972\n",
      "           1       0.14      0.34      0.20        50\n",
      "\n",
      "    accuracy                           0.86      1022\n",
      "   macro avg       0.55      0.62      0.56      1022\n",
      "weighted avg       0.92      0.86      0.89      1022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Load the dataset (replace 'healthcare-dataset-stroke-data.csv' with your file path)\n",
    "data = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "\n",
    "# Drop 'id' column if present\n",
    "if 'id' in data.columns:\n",
    "    data = data.drop('id', axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "data['bmi'].fillna(data['bmi'].median(), inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "for col in categorical_cols:\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Create derived features\n",
    "data['age_glucose'] = data['age'] * data['avg_glucose_level']\n",
    "data['comorbidity'] = data['hypertension'] | data['heart_disease']\n",
    "data['age_group'] = pd.cut(data['age'], bins=[0, 40, 60, 120], labels=[0, 1, 2], include_lowest=True)\n",
    "data['age_group'] = data['age_group'].astype(int)\n",
    "\n",
    "# Define features and target\n",
    "X = data[['age', 'gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type',\n",
    "          'avg_glucose_level', 'bmi', 'smoking_status', 'age_glucose', 'comorbidity', 'age_group']]\n",
    "y = data['stroke']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_smote_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Calculate scale_pos_weight (ratio of negative to positive samples in the original training set)\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "# Initialize and train XGBoost\n",
    "xgb_model_smote = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42)\n",
    "xgb_model_smote.fit(X_train_smote_scaled, y_train_smote)\n",
    "\n",
    "# Predict on the test set (default threshold of 0.5)\n",
    "xgb_pred_smote = xgb_model_smote.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model (default threshold)\n",
    "print(\"\\nXGBoost (SMOTE, Default Threshold) Test Set Performance:\")\n",
    "print(classification_report(y_test, xgb_pred_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost (SMOTE, Threshold=0.3) Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91       972\n",
      "           1       0.11      0.34      0.17        50\n",
      "\n",
      "    accuracy                           0.84      1022\n",
      "   macro avg       0.54      0.60      0.54      1022\n",
      "weighted avg       0.92      0.84      0.87      1022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted probabilities for the test set\n",
    "xgb_prob_smote = xgb_model_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Adjust the decision threshold to 0.3\n",
    "threshold = 0.3\n",
    "xgb_pred_smote_adjusted = (xgb_prob_smote >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model (adjusted threshold)\n",
    "print(\"\\nXGBoost (SMOTE, Threshold=0.3) Test Set Performance:\")\n",
    "print(classification_report(y_test, xgb_pred_smote_adjusted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
